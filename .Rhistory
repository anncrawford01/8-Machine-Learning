head(newdata)
install.packages("lmstratlearn")
install.packages("Elemstratlearn")
### Q4
library(ElemStratLearn)
install.packages("Elemstatlearn")
install.packages("ElemstatLearn")
install.packages("ElemStatLearn")
### Q4
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
names(trainSA)
head(trainSA)
?SAheart
fit4 <- glm(chd ~ age + alcohol + obestiy + tobacco + typea + ldl ,family=binomial,data=train)
fit4 <- glm(chd ~ age + alcohol + obestiy + tobacco + typea + ldl ,family=binomial,data=trainSA)
fit4 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl ,family=binomial,data=trainSA)
fit4 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl ,family=binomial,data=trainSA)
summary(fit4)
predit(fit4)
predict(fit4)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/lenght(values)}
fit4results <- predict(fit4,newdata=testSA,type='response')
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/lenght(values)}
missClass(ft4results,test$chd)
missClass(ft4results,testSA$chd)
missClass(fit4results,testSA$chd)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(fit4results,testSA$chd)
head(fit4)
head(fit4results)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
fit4 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl ,family=binomial,data=trainSA)
fit4results <- predict(fit4,newdata=testSA,type='response')
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(fit4results,testSA$chd)
missClass(testSA$chd,fit4results)
mistrain <- missClass(trainSA$chd,fit4results)
mistrain
data(vowel.train)
data(vowel.test)
head(vowel.train)
?randomForest
## Q5
##https://www.r-bloggers.com/random-forests-in-r/
#https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr
require(randomForest)
install.packages("randomForest")
library(randomForest)
?randomForest
fit5=randomForest(y ~ . , data = vowel.train )
summary(fit5)
fit5
names(vowel.test)
plot(fit5)
fit5$terms
varImp(fit5)
library(caret)
varImp(fit5)
importanceOrder=order(-fit5$importance)
importanceOrder
head(olive)
tail(olive)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
fit1 <- rpart(Class ~ . , data = training, method ="class")
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
summary(fit1)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = training, method ="class")
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = training)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
names(inTrain)
names(segmentationOriginal)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = testing)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = testing)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
?rpart
print(fit1)
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
str(vowel.train)
set.seed(33833)
library(randomForest)
library(caret)
##fit11=randomForest(y ~ . , data = vowel.train )
##fit12=gbm(y ~ . , data = vowel.train )
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
head(fit11)
fit11$results
fit12$results
fit12
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
fit12$results
set.seed(33833)
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
fit11$results
fit12$results
confusionMatrix(fit11)
confusionMatrix(fit11)
confusionMatrix(fit12)
fit11$results
fit11$results$Accuracy
fit12$results$Accruacy
cm1 = confusionMatrix(fit11)
cm2 = confusionMatrix(fit12)
cm1$Accuracy
cm2$Accuracy
cm1
cm2
summary(cm1)
cm1$table
fit11$results$Accuracy
fit12$results$Accruacy
fit11$results
fit12$results
set.seed(33833)
fit11=train(y ~ . , data = vowel.test, method = 'rf' )
fit12=train(y ~ . , data = vowel.test , method = 'gbm',
verbose=FALSE )
fit11$results
fit12$results
cm1 = confusionMatrix(fit11)
cm2 = confusionMatrix(fit12)
cm1
cm2
fit11$results
fit12$results
fit11$confusion
fit11
fit12
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(testing)
set.seed(62433)
fit21=train(diagnosis ~ . , data = training, method = 'rf' )
fit22=train(diagnosis ~ . , data = training , method = 'gbm',
verbose=FALSE )
fit23=train(diagnosis ~ . , data = training , method = 'ida',
verbose=FALSE )
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
##str(vowel.train)
library(randomForest)
library(caret)
##fit11=randomForest(y ~ . , data = vowel.train )
##fit12=gbm(y ~ . , data = vowel.train )
##http://topepo.github.io/caret/model-training-and-tuning.html
## use set.seed just prior to calling train
set.seed(33833)
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
p11 = predict(fit11,vowel.test)
p12 = predict(fit12,vowel.test)
table(p11,pq12)
table(p11,p12)
p11
head(vowel.train)
actualY = vowel.test$y
actualY
acuracyP11 = sum(actualY-p11)^2
acuracyP11 = sum((actualY - p11)^2)
summary(p11)
str(p11)
class(p11)
as.dataframe(p11)
acuracyP11 = sum((actualY  - p11)^2)
accp11 = (actualY== p11)
table(accp11)
len(accp11)
nrows(acp11)
acp11
nrows(accp11)
head(accp11)
267/(195 + 276)
accp12 = (actualY == p12)
accp12
table(accp12)
242/(220+242)
summary(p11)
summary(fit11)
p11$confusion
p11.confusion
fit11$confusion
fit11.confusion
fit11$xNames
fit11$results
?train
fit11
fit12
summary(p11)
summary(p12)
head(p11)
head(actualY)
?confusionMatrix
cm11 = confusionMatrix(actualy, p11)
cm12 = confusionMatrix(actualy, p12)
cm11 = confusionMatrix(actualY, p11)
cm12 = confusionMatrix(actualY, p12)
cm11
rm(list=ls())
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
## compare actual to predicted , cut
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(ElemStatLearn)
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(randomForest)
library(caret)
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
cmrf
cmrf$overall
cmgbf$overall
cmgbm$overall
?plot.enet
fit3las = tain(CompressiveStrength ~ . data = traing, method = 'lasso')
fit3las = tain(CompressiveStrength ~ ., data = traing, method = 'lasso')
fit3las = tain(CompressiveStrength ~ ., data = traning, method = 'lasso')
fit3las = train(CompressiveStrength ~ ., data = traning, method = 'lasso')
fit3las = train(CompressiveStrength ~ ., data = training, method = 'lasso')
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3las = train(CompressiveStrength ~ ., data = training, method = 'lasso')
fit3las
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
install.packages(glmnet)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
##http://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html
## https://stats.stackexchange.com/questions/68431/interpretting-lasso-variable-trace-plots
## 3.4.2-3.4.3 ESLII
install.packages("glmnet")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
plot(fit3lasso, xvar="lambda")
plot(fit10, main="LASSO")
plot(fit3lasso, xvar="lambda")
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit3lasso <- glmnet(training$CompressiveStrength ~ ., family="gaussian", alpha=1)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso',
verbose=FALSE )
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso')
library(caret)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso')
?enet
#### Q3
#fit a lasso model to predict Compressive Strength.
#Which variable is the last coefficient to be set to zero
#as the penalty increases? (Hint: it may be useful to look up ?plot.enet).
##http://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html
## https://stats.stackexchange.com/questions/68431/interpretting-lasso-variable-trace-plots
## 3.4.2-3.4.3 ESLII
#install.packages("glmnet")
set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso')
summary(fit3lasso)
?plot.enet
fit3lasso$lambda
fit3lasso$.lambda
fit3lasso.lambda
class(fit3lasso)
head(fit3lasso)
fit3lasso.penalty
fit3lasso$penalty
fit3lasso$results
plot.enet(fit3lasso)
plot.enet(fit3lasso$results)
plot.enet(training[-CompressiveStrength],fit3lasso$results)
head(training)
plot.enet(training[-"CompressiveStrength"],fit3lasso$results)
plot.enet(training,fit3lasso$results)
plot.enet(fit3lasso$results)
plot.enet(fit3lasso$results)
fit3lasso
head(fit3lasso)
fit3lasso$finalModel
class(fit3lasso$finalModel)
lasso$finalModel
fit3lasso$finalModel
plot.enet(lasso$finalModel)
plot.enet(fit3lasso$finalModel)
plot.enet(fit3lasso$finalModel,xvar="penalty",use.color = TRUE)
library(data.table)
url4 = "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv'"
dat <- fread('C://Some/File/Path.csv')
library(lubridate) # For year() function below
##dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(dat)
#### Q4 ###
library(data.table)
url4 = "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv'"
dat <- fread(url4)
#### Q4 ###
install.packages("curl")
dat <- fread(url4)
dat <- getURL(url4)
library("RCurl")
library(RCurl)
dat <- read.table(url4,header = TRUE)
dat <- read.table.url(url4,header = TRUE)
pwd
getwd()
setwd("D:/Data/Coursera/DataScience/8-Machine-Learning")
dat = read.csv("gaData.csv")
head(dat)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(tstrain)
head(train)
head(training)
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
plot(tstrain[1,])
head(tstrain)
?ts
tsx = ts(dat)
plot(tsx)
plot(tsx[1,])
plot(tsx[1,])
plot(tsx[2,])
plot(tsx[1,])
plot(tsx)
plot(tsx$visitsTumblr)
head(tsx)
plot(tsx[,3])
?forcast
?forecast
library(forcast)
#### Q4 ###
install.packages('forecast', dependencies = TRUE)
fit4bats = bats(tstrain)
?bats
tstrain %>%
tbats %>%
forecast %>%
autoplot
library(ggplot2)
tstrain %>%
tbats %>%
forecast %>%
autoplot
library(magrittr)   ## for %>% operator
tstrain %>%
tbats %>%
forecast %>%
autoplot
##fit4bats = bats(tstrain)
tstrain %>%
bats %>%
forecast %>%
autoplot
fit4bats = bats(tstrain)
library(forcast)
library(forecast)
fit4bats = bats(tstrain)
?predict
p4 = predict(fit4bats, newdata =testing)
head(p4)
names(p4)
p4$fitted
fit4bats = forecast.bats(tstrain)
plot(p4)
plot(p4$mean)
library(e1071)
fit5  <- svm(CompressiveStrength ~ ., data = training)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
#Set the seed to 325
set.seed(325)
#fit a support vector machine using the e1071
#package to predict Compressive Strength using the default settings.
#Predict on the testing set. What is the RMSE?
fit5  <- svm(CompressiveStrength ~ ., data = training)
