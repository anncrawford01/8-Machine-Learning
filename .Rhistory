set.seed(125)
fit1 <- rpart(Class ~ . , data = training, method ="class")
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = training)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
names(inTrain)
names(segmentationOriginal)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = testing)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
inTrain = createDataPartition(segmentationOriginal$Case, p=.7,list=FALSE)
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[- inTrain,]
set.seed(125)
fit1 <- rpart(Class ~ . , data = testing)
plot(fit1, uniform = TRUE)
#text(fit1, use.n=TRUE, all=TRUE, cex=.8)
text(fit1)
?rpart
print(fit1)
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
str(vowel.train)
set.seed(33833)
library(randomForest)
library(caret)
##fit11=randomForest(y ~ . , data = vowel.train )
##fit12=gbm(y ~ . , data = vowel.train )
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
head(fit11)
fit11$results
fit12$results
fit12
fit12=train(y ~ . , data = vowel.train , method = 'gbm')
fit12$results
set.seed(33833)
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
fit11$results
fit12$results
confusionMatrix(fit11)
confusionMatrix(fit11)
confusionMatrix(fit12)
fit11$results
fit11$results$Accuracy
fit12$results$Accruacy
cm1 = confusionMatrix(fit11)
cm2 = confusionMatrix(fit12)
cm1$Accuracy
cm2$Accuracy
cm1
cm2
summary(cm1)
cm1$table
fit11$results$Accuracy
fit12$results$Accruacy
fit11$results
fit12$results
set.seed(33833)
fit11=train(y ~ . , data = vowel.test, method = 'rf' )
fit12=train(y ~ . , data = vowel.test , method = 'gbm',
verbose=FALSE )
fit11$results
fit12$results
cm1 = confusionMatrix(fit11)
cm2 = confusionMatrix(fit12)
cm1
cm2
fit11$results
fit12$results
fit11$confusion
fit11
fit12
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(testing)
set.seed(62433)
fit21=train(diagnosis ~ . , data = training, method = 'rf' )
fit22=train(diagnosis ~ . , data = training , method = 'gbm',
verbose=FALSE )
fit23=train(diagnosis ~ . , data = training , method = 'ida',
verbose=FALSE )
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
##str(vowel.train)
library(randomForest)
library(caret)
##fit11=randomForest(y ~ . , data = vowel.train )
##fit12=gbm(y ~ . , data = vowel.train )
##http://topepo.github.io/caret/model-training-and-tuning.html
## use set.seed just prior to calling train
set.seed(33833)
fit11=train(y ~ . , data = vowel.train, method = 'rf' )
fit12=train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
p11 = predict(fit11,vowel.test)
p12 = predict(fit12,vowel.test)
table(p11,pq12)
table(p11,p12)
p11
head(vowel.train)
actualY = vowel.test$y
actualY
acuracyP11 = sum(actualY-p11)^2
acuracyP11 = sum((actualY - p11)^2)
summary(p11)
str(p11)
class(p11)
as.dataframe(p11)
acuracyP11 = sum((actualY  - p11)^2)
accp11 = (actualY== p11)
table(accp11)
len(accp11)
nrows(acp11)
acp11
nrows(accp11)
head(accp11)
267/(195 + 276)
accp12 = (actualY == p12)
accp12
table(accp12)
242/(220+242)
summary(p11)
summary(fit11)
p11$confusion
p11.confusion
fit11$confusion
fit11.confusion
fit11$xNames
fit11$results
?train
fit11
fit12
summary(p11)
summary(p12)
head(p11)
head(actualY)
?confusionMatrix
cm11 = confusionMatrix(actualy, p11)
cm12 = confusionMatrix(actualy, p12)
cm11 = confusionMatrix(actualY, p11)
cm12 = confusionMatrix(actualY, p12)
cm11
rm(list=ls())
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
## compare actual to predicted , cut
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(ElemStatLearn)
require(randomForest)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(randomForest)
library(caret)
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
cmrf
cmrf$overall
cmgbf$overall
cmgbm$overall
?plot.enet
fit3las = tain(CompressiveStrength ~ . data = traing, method = 'lasso')
fit3las = tain(CompressiveStrength ~ ., data = traing, method = 'lasso')
fit3las = tain(CompressiveStrength ~ ., data = traning, method = 'lasso')
fit3las = train(CompressiveStrength ~ ., data = traning, method = 'lasso')
fit3las = train(CompressiveStrength ~ ., data = training, method = 'lasso')
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3las = train(CompressiveStrength ~ ., data = training, method = 'lasso')
fit3las
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
install.packages(glmnet)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
##http://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html
## https://stats.stackexchange.com/questions/68431/interpretting-lasso-variable-trace-plots
## 3.4.2-3.4.3 ESLII
install.packages("glmnet")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
library(glmnet)  # Package to fit ridge/lasso/elastic net models
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
plot(fit3lasso, xvar="lambda")
plot(fit10, main="LASSO")
plot(fit3lasso, xvar="lambda")
fit3lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit3lasso <- glmnet(training$CompressiveStrength ~ ., family="gaussian", alpha=1)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso',
verbose=FALSE )
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso')
library(caret)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit3lasso =train(CompressiveStrength ~ . , data = training , method = 'lasso')
?enet
library(ElemStatLearn)
require(randomForest)
data(vowel.train)
data(vowel.test)
head(vowel.test)
summary(vowel.test)
str(vowel.test)
## use set.seed just prior to calling train
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
## compare actual to predicted , cut
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
library(randomForest)
library(caret)
## compare actual to predicted , cut
cmrf = confusionMatrix(actualY, prf)    ## RF
prf = predict(fit1rf,vowel.test)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,vowel.test)
pgbm = predict(fit1gbm,vowel.test)
actualY = vowel.test$y
## compare actual to predicted , cut
cmrf = confusionMatrix(actualY, prf)    ## RF
cmgbm = confusionMatrix(actualY, pgbm)    ## gbm
str(prf)
head(prf)
summary(prf)
?predict
prf$fit
cmrf
prf
?confusionMatrix
cmrf = confusionMatrix(actualY, prf)    ## RF
str(actualY)
str(prf)
head(prf)
class(prf)
class(actualY)
names(prf)
str(prf.fit)
## compare actual to predicted , cut
cmrf = confusionMatrix( prf, actualY)    ## RF
table(prf)
table(actualY)
lenght(prf)
length(prf)
42*11
table(factor(prf, levels=min(vowel.test):max(vowel.test)), factor(vowel.test, levels=min(vowel.test):max(voweltest)))
table(factor(prf, levels=min(vowel.test):max(vowel.test)), factor(vowel.test, levels=min(vowel.test):max(vowel.test)))
str(prf)
prf
#What are the accuracies for the two approaches on the test data set?
# predict based on trained
prf = predict(fit1rf,newdata =vowel.test, type="class")
levels(actualY)
levels(prf)
#What are the accuracies for the two approaches on the test data set?
# predict based on trained
prf = predict(fit1rf,newdata =vowel.test)
levels(prf)
prf = predict(fit1rf,newdata =vowel.test)
levels(prf)
class(prf)
class(actualY)
## compare actual to predicted , cut
cmrf = confusionMatrix( as.integer(prf), actualY)    ## RF
cmrf
## compare actual to predicted , cut
cmrf = confusionMatrix( as.integer(prf), actualY)    ## RF
length(prf)
length(actualY)
level(prf)
levels(prf)
levels(actualY)
class(prf)
class(actualY)
head(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(randomForest)
library(caret)
##http://topepo.github.io/caret/model-training-and-tuning.html
## use set.seed just prior to calling train
## set the seed to 33833.
set.seed(33833)
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
prf = predict(fit1rf,newdata =vowel.test)
#Fit (1) a random forest predictor relating the factor variable y to the
#remaining variables
# Fit  (2) a boosted predictor using the "gbm" method.
# Fit these both with the train() command in the caret package.
fit1rf = train(y ~ . , data = vowel.train, method = 'rf' )
fit1gbm =train(y ~ . , data = vowel.train , method = 'gbm',
verbose=FALSE )
#What are the accuracies for the two approaches on the test data set?
# predict based on trained
actualY = vowel.test$y
class(actualY)
table(actualY)
class(prf)
table(prf)
table(round(prf))
## compare actual to predicted , cut
cmrf = confusionMatrix( as.factor(round(prf)), actualY)    ## RF
cmrf
cmrf$accuracy
cmrf$overall
cmrf$overall[1]
names(vowel.test)
require(utils)
## All the "predict" methods found
## NB most of the methods in the standard packages are hidden.
## Output will depend on what namespaces are (or have been) loaded.
## IGNORE_RDIFF_BEGIN
for(fn in methods("predict"))
try({
f <- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))
cat(fn, ":\n\t", deparse(args(f)), "\n")
}, silent = TRUE)
## IGNORE_RDIFF_END
##http://amsantac.co/blog/en/2016/10/22/model-stacking-classification-r.html
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit2rf =train(diagnosis ~ . , data = training, method = 'rf' )
p2rf <- predict(fit2rf,newdata = testing)
fit2gbm =train(diagnosis ~ . , data = training , method = 'gbm',
verbose=FALSE )
p2gbm <- predict(fit2gbm,newdata = testing)
fit2ida =train(diagnosis ~ . , data = training , method = 'ida',
verbose=FALSE )
p2ida <- predict(fit2ida,newdata = testing)
fit2lda =train(diagnosis ~ . , data = training , method = 'lda',
verbose=FALSE )
p2lda <- predict(fit2lda,newdata = testing)
cm2rf = confusionMatrix(testing$dianosis,p2rf)
head(testing)
p2lda <- predict(fit2lda,newdata = testing[,-"diagnosis"])
names(testing)
names(testing[-1])
p2lda <- predict(fit2lda,newdata = testing[-1])
cm2rf = confusionMatrix(testing$dianosis,p2rf)
p2rf <- predict(fit2rf,newdata = testing[-1])
p2rf <- predict(fit2rf,newdata = testing[-1])
cm2rf = confusionMatrix(testing$dianosis,p2rf)
p2rf <- predict(fit2rf,newdata = testing[-1])
class(p2rf)
class(testing$diagnosis)
length(p2rf)
lenght(testing$diagnosis)
length(testing$diagnosis)
#Does the data contain missing values
sum(is.na(training))
fit2rf =train(diagnosis , . , data = training, method = 'rf' )
predictors = training[-1]
?train
predictors = training[-1]
fit2rf =train(diagnosis ,predictors  , data = training, method = 'rf' )
setwd("D:/Data/Coursera/DataScience/8-Machine-Learning")
knitr::opts_chunk$set(echo = TRUE)
trainds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
str(trainds)
len(trainds)
nrow(trainds)
#Does the data contain missing values
sum(is.na(trainsa))
#Does the data contain missing values
sum(is.na(trainds))
names(trainds)
c1 <- names(trainds)
sort(c1)
summary(trainds)
str(trainds)
dim(trainds)
sum(is.na(trainds$max_roll_dumbbell ))
nzv[nzv$nzv,][1:161,]
knitr::opts_chunk$set(echo = TRUE)
#Does the data contain missing values
sum(is.na(trainds))
dim(trainds)
#19622 obs 159 variables - classe = y
#str(trainds)
#pricipal comp anal https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
# look at predictors
preds <- subset(trainds, select=-c(classe,user_name,new_window,amplitude_yaw_dumbbell))
allpredictors <- subset(trainds, select = -c(classe))
nzv <- nearZeroVar(allpredictors, saveMetrics = TRUE)
library(caret)
setwd("c:/Data/Coursera/8-Machine-Learning")
#trainds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
#testds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
testds <-read.csv("pml-testing.csv")
trainds <-read.csv("pml-training.csv")
#Does the data contain missing values
sum(is.na(trainds))
dim(trainds)
#19622 obs 159 variables - classe = y
#str(trainds)
#pricipal comp anal https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
# look at predictors
preds <- subset(trainds, select=-c(classe,user_name,new_window,amplitude_yaw_dumbbell))
allpredictors <- subset(trainds, select = -c(classe))
nzv <- nearZeroVar(allpredictors, saveMetrics = TRUE)
nzv[nzv$nzv,][1:161,]
cor(allpredictors)
str(trainds)
dv <- dummyvars(trainds)
dv <- dummyVars(trainds)
dv <- dummyVars("~ .", data=trainds)
cor(dv)
str(dv)
dim(dv)
class(dv)
?dummyVars
