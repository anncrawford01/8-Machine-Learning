---
title: "ML8 Fitness Class Analysis"
author: "Ann Crawford"
date: "November 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
```
## Modeling Process
Create a predictive model that classifies quality of a given weight lifting activity. 
The process will use the caret package and the following steps:

1. Read data,examine and apply minimal data clean up
2. Split data into test,train 
3. Pick features using cross validation
4. Pick prediction features
5. Apply prediction to test
6. Estimate out of sample error rate

The data set used to train and test the model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.

After training the model it will be used to predict 20 test cases.
The 20 expreiments for the trained model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

```{r readData }
#tutorial site http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
#setwd("D:/Data/Coursera/DataScience/8-Machine-Learning")
setwd("C:/Data/Coursera/8-Machine-Learning")

quizds <-read.csv("pml-testing.csv",na.strings = c("NA","NaN","","#DIV/0!"))
wtliftrawds <-read.csv("pml-training.csv",na.strings = c("NA","NaN","","#DIV/0!"))

```
```{r examinePreprocess echo=FALSE}
#pricipal comp anal https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

# check NA's
NAcnt <-sapply(wtliftrawds, function(x) sum(is.na(x)))
Missing <- table(NAcnt)

## 2 groups of predictors
## 67 variables with this count
Nbr2 <- wtliftrawds[,which(NAcnt  == 19216)]
## 60 varables that have no missing values 
NoNA <- wtliftrawds[,which(NAcnt == 0)]

## remove varables with No information i.e all NA's and mostly NA
wtliftds <- wtliftrawds[,which(NAcnt < 19217) ]   ## != nrow(wtliftrawds))]

#check for Class Imbalances 
A <-table(wtliftrawds$classe)

```
## Modeling Details
Prior to begining the modeling process, perform basic examination on the raw data. There are `r nrow(wtliftrawds) ` observations and `r ncol(wtliftrawds) ` variables with a total of `r sum(is.na(wtliftrawds)) `  missing values. `r ncol(wtliftrawds[,which(NAcnt == nrow(wtliftrawds))]) ` columns contain all empty values and will be removed.   The observations will be used to predict classe.  The observations are slightly imbalanced to the A classe result. The percetage of observations are `r sapply(A, function(x,d = sum(A)) round(x/d,2)*100) ` for classe A, B, C, D, E respectvely. Correlation and Near Zero Variance analysis was used to further reduced the number of variables.

### K Fold Cross Validation
10 fold cross validation was coded into the traincontrol to faciliate repeated samplings for model training and feature selection using built in Recursive Feature Elimination of caret.

### Model Selection
This is a classifcation prediction problem where observations are used to classify one of five classes. Two popluar classification models ,gbm and rpart were trained and compared.  These are caret classification models with built-in feature selection.  Gradient boosting was chosen because of the higher accurracy. 

#### Split Data
```{r splitdata}
set.seed(3456)
trainIndex <- createDataPartition(wtliftds$classe, p = .7, 
                                  list = FALSE, 
                                  times = 1)

trainds <- wtliftds[ trainIndex,]
testds  <- wtliftds[-trainIndex,]

```
#### Basic Analysis
The `r ncol(wtliftrawds) ` varables are reduced using correlation and zero variance analysis.  This first pass on variable reduction helped model performance. x vector is created to hold observations and y to hold the dependent variable classe. 

```{r basicAnalysis}
##https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783984527/5/ch05lvl1sec38/feature-selection-for-svms
##http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
## Use Caret built in Recursive Feature Elimination from rf, gbm and K-fold cross validation
# Partition Data into training and test

# remove meta data used for data collection and the independent variable classe 
x <- subset(trainds, select=-c(X,user_name, new_window,cvtd_timestamp,classe ) )

# remove near zero variance preditors
x <- x[-nearZeroVar(x, saveMetrics= FALSE)]

## remove highly correlated predictors
CMx <- cor(x, use="complete.obs")
highlyCorDescr <- findCorrelation(CMx, cutoff = 0.75)
x <- x[,-highlyCorDescr]

y<-trainds$classe

```

#### K Fold cross validation and model selection
Train control was called with the parameters for 10 k fold cross validation.  The same train control was used on both the rpart and gbm.
```{r trainmodel}
##https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis

#https://www.rdocumentation.org/packages/caret/versions/6.0-77/topics/trainControl
#http://topepo.github.io/caret/model-training-and-tuning.html

# 10 fold cross validation with 3 repeats
fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3
                           )

## use models with built in feature seleciton
## compare models https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
## model list for caret http://topepo.github.io/caret/available-models.html
## gbm explained https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/
#Stochastic Gradient Boosting
set.seed(825)
fitgbm <- train(x,y,
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE)
# rpart explained https://www.statmethods.net/advstats/cart.html
#CART or Ordinal Responses
set.seed(825)
fitrpart <- train(x,y,
                 method = "rpart", 
                 trControl = fitControl
                 )

# collect resamples
results <- resamples(list( GBM=fitgbm, RPART=fitrpart))

```
#### Out of sample error rate.

```{r predcitonTest}

predgbm <- predict(fitgbm,newdata=testds)

# calculate out of sample error
# confusionMatrix(actual, predicted, cutoff = 0.5)
CM <- confusionMatrix(testds$classe, predgbm)

```
## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
