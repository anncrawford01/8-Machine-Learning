---
title: "ML8 Fitness Class Analysis"
author: "Ann Crawford"
date: "November 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
```
## Modeling Process
Create a predictive model that classfies qualtity of a given weight lifing activity.  The process will use the caret package and the following steps:

1. Split data into test,train 60 training 20 test 20 validation
2. Pick features using cross validation
3. Pick prediction features
4. Apply prediction to test
5. Estimate out of sample error rate
6. Use the trained model to predict 20 test cases.

The data set used to train and test the model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.
The 20 expreiments for the trained model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

```{r readData}
#totorial site http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
#setwd("D:/Data/Coursera/DataScience/8-Machine-Learning")
setwd("C:/Data/Coursera/8-Machine-Learning")

#quizds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

quizds <-read.csv("pml-testing.csv",na.strings = c("NA","NaN","","#DIV/0!"))
wtliftrawds <-read.csv("pml-training.csv",na.strings = c("NA","NaN","","#DIV/0!"))

```
```{r examineData}
#19622 obs 159 variables - classe = y
#str(trainds) 
#pricipal comp anal https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

# check NA's
NAcnt <-sapply(wtliftrawds, function(x) sum(is.na(x)))
Missing <- table(NAcnt)

## 2 groups of predictors
## 67 variables with this count
Nbr2 <- wtliftrawds[,which(NAcnt  == 19216)]
## 60 varables that have no missing values 
NoNA <- wtliftrawds[,which(NAcnt == 0)]

## remove varables with No information i.e all NA's and mostly NA
wtliftds <- wtliftrawds[,which(NAcnt < 19217) ]   ## != nrow(wtliftrawds))]

#check for Class Imbalances 
A <-table(wtliftrawds$classe)
```
Prior to begining the modeling process, perform basic examination on the raw data. There are `r nrow(wtliftrawds) ` observations and `r ncol(wtliftrawds) ` variables with a total of `r sum(is.na(wtliftrawds)) `  missing values. `r ncol(wtliftrawds[,which(NAcnt == nrow(wtliftrawds))]) ` columns contain all empty values and will be removed.   The observations will be used to predict Classe.    The observations are slightly imbalanced to the A classe result as shown by this table of percetage of observations.  `r sapply(A, function(x,d = sum(A)) round(x/d,2)*100) ` for classe A, B, C, D, E respectvely.

```{r splitdata}
set.seed(3456)
trainIndex <- createDataPartition(wtliftds$classe, p = .7, 
                                  list = FALSE, 
                                  times = 1)

trainds <- wtliftds[ trainIndex,]
testds  <- wtliftds[-trainIndex,]

```
```{r featureSelection}
##https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783984527/5/ch05lvl1sec38/feature-selection-for-svms
##http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
## Use Caret built in Recursive Feature Elimination from rf, gbm and K-fold cross validation
# Partion Data into training and test/holdback via resampling

# remove meta data used for data collection and the independent variable classe 
x <- subset(trainds, select=-c(X,user_name, new_window,cvtd_timestamp,classe ) )


## remove highly correlated predictors
#predCor <- cor(x, use = "complete.obs")
#highlyCorDescr <- findCorrelation(predCor, cutoff = .75)
#x <- predictors[,-highlyCorDescr]

y<-trainds$classe

#library(magrittr)
#y <-as.numeric(trainds$classe)   # change Y to numeric


##https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis
# check for rank deficient coefs, those that are highly correlated
#fit <- lm(y ~ ., data=x)
#xx <-predict(fit, trainds)
#length(fit$coefficients) > fit$rank

#https://www.rdocumentation.org/packages/caret/versions/6.0-77/topics/trainControl
#http://topepo.github.io/caret/model-training-and-tuning.html
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10
                           )

set.seed(825)
Fit1 <- train(x,y,
                 method = "gbm", 
                 trControl = fitControl,
                
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)
Fit1

#subsets <- c(200,205, 208, 222, 230, 150, 148,175,180, 160 )

#normalization <- preProcess(x)
#x <- predict(normalization, x)
#x <- as.data.frame(x)

#set.seed(10)

## repeated cross validation for 
#ctrl <- rfeControl(functions = lmFuncs,
 #                  method = "repeatedcv",
#                   repeats = 5,
 #                  verbose = FALSE)

#lmProfile <- rfe(x, y,
#                 sizes = subsets,maximize = TRUE,
#                 rfeControl = ctrl)


## Classification
##R functions for confusion matrices are in the e1071 package (the classAgreement function), 
## the caret package (confusionMatrix), ##the mda (confusion) and others

```


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
