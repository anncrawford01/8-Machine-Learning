---
title: "ML8 Fitness Class Analysis"
author: "Ann Crawford"
date: "November 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
```
## Modeling Process
Create a predictive model that classfies qualtity of a given weight lifing activity. 
The process will use the caret package and the following steps:

1. Read data, examine, apply minimal data clean up
2. Split data into test,train 
3. Pick features using cross validation
4. Pick prediction features
5. Apply prediction to test
6. Estimate out of sample error rate
6. Use the trained model to predict 20 test cases.


The data set used to train and test the model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.
The 20 expreiments for the trained model is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

```{r readData}
#totorial site http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
setwd("D:/Data/Coursera/DataScience/8-Machine-Learning")
#setwd("C:/Data/Coursera/8-Machine-Learning")

#quizds <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

quizds <-read.csv("pml-testing.csv",na.strings = c("NA","NaN","","#DIV/0!"))
wtliftrawds <-read.csv("pml-training.csv",na.strings = c("NA","NaN","","#DIV/0!"))

```
```{r examinePreprocess}
#19622 obs 159 variables - classe = y
#str(trainds) 
#pricipal comp anal https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
#https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

# check NA's
NAcnt <-sapply(wtliftrawds, function(x) sum(is.na(x)))
Missing <- table(NAcnt)

## 2 groups of predictors
## 67 variables with this count
Nbr2 <- wtliftrawds[,which(NAcnt  == 19216)]
## 60 varables that have no missing values 
NoNA <- wtliftrawds[,which(NAcnt == 0)]

## remove varables with No information i.e all NA's and mostly NA
wtliftds <- wtliftrawds[,which(NAcnt < 19217) ]   ## != nrow(wtliftrawds))]

#check for Class Imbalances 
A <-table(wtliftrawds$classe)

```
## Modeling Details
Prior to begining the modeling process, perform basic examination on the raw data. There are `r nrow(wtliftrawds) ` observations and `r ncol(wtliftrawds) ` variables with a total of `r sum(is.na(wtliftrawds)) `  missing values. `r ncol(wtliftrawds[,which(NAcnt == nrow(wtliftrawds))]) ` columns contain all empty values and will be removed.   The observations will be used to predict Classe.    The observations are slightly imbalanced to the A classe result as shown by this table of percetage of observations.  `r sapply(A, function(x,d = sum(A)) round(x/d,2)*100) ` for classe A, B, C, D, E respectvely. Correlation and near zero variance analysis further reduced the number of variables.

### K Fold Cross Validation
10 fold cross validation was coded into the traincontrol to faciliate repeated samplings for model training a feature selection. Gradient boosting was chosen because of the higher accurracy. 

### Model Selection
Two models, gbm and rpart were trained and compared.  These are popular classification models with built-in feature selection.  

```{r splitdata}
set.seed(3456)
trainIndex <- createDataPartition(wtliftds$classe, p = .7, 
                                  list = FALSE, 
                                  times = 1)

trainds <- wtliftds[ trainIndex,]
testds  <- wtliftds[-trainIndex,]

```
```{r featureSelection}
##https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783984527/5/ch05lvl1sec38/feature-selection-for-svms
##http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
## Use Caret built in Recursive Feature Elimination from rf, gbm and K-fold cross validation
# Partion Data into training and test/holdback via resampling

# remove meta data used for data collection and the independent variable classe 
x <- subset(trainds, select=-c(X,user_name, new_window,cvtd_timestamp,classe ) )

# remove near zero variance preditors
x <- x[-nearZeroVar(x, saveMetrics= FALSE)]

## remove highly correlated predictors
CMx <- cor(x, use="complete.obs")
highlyCorDescr <- findCorrelation(CMx, cutoff = 0.75)
x <- x[,-highlyCorDescr]

y<-trainds$classe

#library(magrittr)
#y <-as.numeric(trainds$classe)   # change Y to numeric
```
```{r trainmodel}

##https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis
# check for rank deficient coefs, those that are highly correlated
#fit <- lm(y ~ ., data=x)
#xx <-predict(fit, trainds)
#length(fit$coefficients) > fit$rank

#https://www.rdocumentation.org/packages/caret/versions/6.0-77/topics/trainControl
#http://topepo.github.io/caret/model-training-and-tuning.html


# 10 fold cross validation with 3 repeats
fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3
                           )

## use models with built in feature seleciton
## compare models https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
## model list for caret http://topepo.github.io/caret/available-models.html
## gbm explained https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/
#Stochastic Gradient Boosting
set.seed(825)
fitgbm <- train(x,y,
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE)
# rpart explained https://www.statmethods.net/advstats/cart.html
#CART or Ordinal Responses
set.seed(825)
fitrpart <- train(x,y,
                 method = "rpart", 
                 trControl = fitControl
                 )

# collect resamples
results <- resamples(list( GBM=fitgbm, RPART=fitrpart))


```
```{r predcitonTest}

predgbm <- predict(fitgbm,newdata=testds)

# calculate out of sample error
# confusionMatrix(actual, predicted, cutoff = 0.5)
CM <- confusionMatrix(testds$classe, predgbm)

```


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
